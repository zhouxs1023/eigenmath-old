\documentclass[11pt]{article}
\title{Eigenmath Manual}
%\author{George Weigt}
\date{\today}
\usepackage{graphicx}
\usepackage{makeidx}
\makeindex

\begin{document}

\maketitle

\newpage

\include{syntax}

\newpage

\include{nabokov}

\include{avogadro}

\include{zerozero}

\include{geometric-series}

\include{units-of-measure}

\include{draw}

\include{scripting}





\newpage

\index{linear algebra}

\noindent
$dot$ is used to multiply vectors and matrices.
The following example shows how to use $dot$ and $inv$ to solve for
$\bf X$ in $\bf AX=B$.

\medskip
{\tt A=((3.8,7.2),(1.3,-0.9))}

{\tt B=(16.5,-22.1)}

{\tt X=dot(inv(A),B)}

{\tt X}

$$\left(\matrix{-11.2887\cr8.24961}\right)$$

\medskip
\noindent
One might wonder why the $dot$ function is necessary.
Why not simply use $X=inv(A)*B$ like scalar multiplication?
The reason is that the software normally reorders factors internally to optimize processing.
For example, $inv(A)*B$ in symbolic form is changed to $B*inv(A)$ internally.
Since the dot product is not commutative, this reordering would give the wrong result.
Using a function to do the multiply avoids the problem because
function arguments are not reordered.

\medskip
\noindent
It should be noted that $dot$ can have more than two arguments.
For example, $dot(A,B,C)$ can be used for the dot product of three tensors.

\newpage

\noindent
The following example demonstrates the relation
${\bf A}^{-1}=\mathop{\rm adj}{\bf A}/\mathop{\rm det}{\bf A}$.

\medskip
\verb$A=((a,b),(c,d))$

\medskip
\verb$inv(A)$
$$\left(\matrix{
\displaystyle{d\over ad-bc} & \displaystyle{-{b\over ad-bc}}\cr
\cr
\displaystyle{-{c\over ad-bc}} & \displaystyle{a\over ad-bc}\cr
}\right)$$

\medskip
\verb$adj(A)$
$$\left(\matrix{
d & -b\cr
-c & a\cr
}\right)$$

\medskip
\verb$det(A)$
$$ad-bc$$

\medskip
\verb$inv(A)-adj(A)/det(A)$
$$\left(\matrix{
0 & 0\cr
0 & 0\cr
}\right)$$

\medskip
\noindent
Sometimes a calculation will be simpler if it can be reorganized to use $adj$ instead of $inv$.
The main idea is to try to prevent the determinant from appearing as a divisor.
For example, suppose for matrices $\bf A$ and $\bf B$ you want to check that
$${\bf A}-{\bf B}^{-1}=0$$
Depending on the complexity of $\mathop{\rm det}\bf B$, the software
may not be able to find a simplification that yields zero.
Should that occur, the following alternative can be tried.
$$(\mathop{\rm det}{\bf B})\cdot{\bf A}-\mathop{\rm adj}{\bf B}=0$$

\newpage

\noindent
The adjunct of a matrix is related to the cofactors as follows.

\medskip
\verb$A=((a,b),(c,d))$

\verb$C=((0,0),(0,0))$

\verb$C[1,1]=cofactor(A,1,1)$

\verb$C[1,2]=cofactor(A,1,2)$

\verb$C[2,1]=cofactor(A,2,1)$

\verb$C[2,2]=cofactor(A,2,2)$

\verb$C$

$$C=\left(\matrix{d&-c\cr -b&a}\right)$$

\verb$adj(A)-transpose(C)$

$$\left(\matrix{0&0\cr0&0\cr}\right)$$

\include{complex}

\section*{Derivatives}
\index{derivative}
$d(f,x)$ returns the derivative of $f$ with respect to $x$.
The $x$ can be omitted for expressions in $x$.

\medskip
\verb$d(x^2)$
$$2x$$

\bigskip
\noindent
The following table summarizes the various ways to obtain multiderivatives.

\begin{center}
\begin{tabular}{cllllll}
%& & & & {\it alternate form} \\
%\\
$\displaystyle{\partial^2f\over\partial x^2}$ & & \verb$d(f,x,x)$ & & \verb$d(f,x,2)$ \\
\\
$\displaystyle{\partial^2f\over\partial x\,\partial y}$ & & \verb$d(f,x,y)$ \\
\\
$\displaystyle{\partial^{m+n+\cdot\cdot\cdot} f\over\partial x^m\,\partial y^n\cdots}$ & &
\verb$d(f,x,...,y,...)$ & & \verb$d(f,x,m,y,n,...)$ \\
\end{tabular}
\end{center}

%\medskip
%\verb$r=sqrt(x^2+y^2)$

%\verb$d(r,x,y)$
%$${-{xy\over(x^2+y^2)^{3/2}}}$$

\index{gradient}

\noindent
The gradient of $f$ is obtained by using a vector for $x$ in $d(f,x)$.

\medskip
\verb$r=sqrt(x^2+y^2)$

\verb$d(r,(x,y))$
$$\left(\matrix{
\displaystyle{{x\over(x^2+y^2)^{1/2}}}\cr
\cr
\displaystyle{{y\over(x^2+y^2)^{1/2}}}\cr
}\right)$$

\medskip
\noindent
The $f$ in $d(f,x)$ can be a tensor function.
Gradient raises the rank by one.

\medskip
\verb$F=(x+2y,3x+4y)$

\verb$X=(x,y)$

\verb$d(F,X)$
$$\left(\matrix{1&2\cr3&4}\right)$$

\noindent
The function $f$ in $d(f)$ does not have to be defined.
It can be a template function with just a name and an argument list.
Eigenmath checks the argument list to figure out what to do.
For example, $d(f(x),x)$ evaluates to itself because $f$ depends on $x$.
However, $d(f(x),y)$ evaluates to zero because $f$ does not depend on $y$.

\medskip
\verb$d(f(x),x)$
$$\partial(f(x),x)$$

\verb$d(f(x),y)$
$$0$$

\verb$d(f(x,y),y)$
$$\partial(f(x,y),y)$$

\verb$d(f(),t)$
$$\partial(f(),t)$$

\medskip
\noindent
As the final example shows, an empty argument list causes
$d(f)$ to always evaluate to itself, regardless
of the second argument.

\medskip
\noindent
Template functions are useful for experimenting with differential forms.
For example, let us check the identity
$$\mathop{\rm div}(\mathop{\rm curl}{\bf F})=0$$
for an arbitrary vector function $\bf F$.

\medskip
\verb$F=(F1(x,y,z),F2(x,y,z),F3(x,y,z))$

\verb$curl(U)=(d(U[3],y)-d(U[2],z),d(U[1],z)-d(U[3],x),d(U[2],x)-d(U[1],y))$

\verb$div(U)=d(U[1],x)+d(U[2],y)+d(U[3],z)$

\verb$div(curl(F))$
$$0$$

\include{integrals}

\include{integral-trick}

\include{fund-thm-of-calculus}

\include{line-integral}

\include{surface-area}

\include{surface-integral}

\include{greens-theorem}

\include{stokes-theorem}

\include{francois-viete}

\include{curl-in-tensor-form}

\include{qho}

\include{hydrogen-wavefunctions}

\include{space-shuttle-and-corvette}

\newpage

\include{list-of-functions}

\printindex
\end{document}
